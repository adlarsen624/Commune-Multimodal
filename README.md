# Commune Multimodal

 <img src="/frontend/public/gif/logo/commune.gif" alt="Alt Text" width="300" height="300"> 
 
Based on the open-source multi-modal model [Commune](https://github.com/commune-ai), we create various **visual instruction** data with open datasets, including VQA, Image Captioning, Visual Reasoning, Text OCR, and Visual Dialogue. Additionally, we also train the language model component of Commune using only **language-only instruction** data.

**1. Frontend Running**

# Commune Multimodal project

This is a Next.js project

## Getting Started

To get started with this project, follow the steps below:

1. Clone the repository:

```
git clone https://github.com/commune-ai-multimodal/commune-multimodal.git
```

2. Install the dependencies:

```
cd frontend
npm install
```

3. Run the development server:

```
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser to see the application running.

## Available Scripts

In the project directory, you can run the following scripts:

### `npm run dev`

Runs the app in development mode.<br />
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.<br />
You will also see any lint errors in the console.

### `npm run build`

Builds the app for production to the `.next` folder.<br />
It correctly bundles React in production mode and optimizes the build for the best performance.

### `npm start`

Starts the app in production mode.<br />
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

## Learn More

To learn more about Next.js, check out the [Next.js documentation](https://nextjs.org/docs).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
